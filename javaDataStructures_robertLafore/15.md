15
==

When to Use What
----------------

> In this chapter we briefly summarize what we've learned so far, with
> an eye toward deciding what data structure or algorithm to use in a
> particular situation.
>
> This chapter comes with the usual caveats. Of necessity, it's very
> general. Every real-world situation is unique, so what we say here may
> not be the right answer to your problem. This chapter is divided into
> these somewhat arbitrary sections:

-   General-purpose data structures: arrays, linked lists, trees, hash
    tables

-   Specialized data structures: stacks, queues, priority queues, graphs

-   Sorting: insertion sort, Shellsort, quicksort, merge- sort, heapsort

-   Graphs: adjacency matrix, adjacency list

-   External storage: sequential storage, indexed files, B-trees,
    hashing

##### IN THIS CHAPTER

-   General-Purpose Data Structures

-   Special-Purpose Data Structures

-   Sorting

-   Graphs

-   External Storage

> **[NOTE ]{.underline}**
>
> For detailed information on these topics, refer to the individual
> chapters in this book.

#### General-Purpose Data Structures

> If you need to store real-world data such as personnel records,
> inventories, contact lists, or sales data, you need a general-purpose
> data structure. The structures of this type that we've discussed in
> this book are arrays, linked lists, trees, and hash tables. We call
> these general-purpose data
>
> 718 **CHAPTER 15** When to Use What
>
> structures because they are used to store and retrieve data using key
> values. This works for general-purpose database programs (as opposed
> to specialized structures such as stacks, which allow access to only
> certain data items).
>
> Which of these general-purpose data structures is appropriate for a
> given problem? Figure 15.1 shows a first approximation to this
> question. However, there are many factors besides those shown in the
> figure. For more detail, we'll explore some general considerations
> first and then zero in on the individual structures.

+-------------------+------+
|                   | > No |
+===================+======+
| > Unordered array |
+-------------------+------+

> ***FIGURE 15.1*** Relationship of general-purpose data structures.

##### Speed and Algorithms

> The general-purpose data structures can be roughly arranged in terms
> of speed: Arrays and linked lists are slow, trees are fairly fast, and
> hash tables are very fast.
>
> However, don't draw the conclusion from Figure 15.1 that it's always
> best to use the fastest structures. There's a penalty for using them.
> First, they are---in varying degrees---more complex to program than
> the array and linked list. Also, hash tables require you to know in
> advance about how much data can be stored, and they don't use memory
> very efficiently. Ordinary binary trees will revert to slow O(N)
> operation for ordered data, and balanced trees, which avoid this
> problem, are difficult to program.
>
> General-Purpose Data Structures 719

####### Processor Speed: A Moving Target

> The fast structures come with penalties, and another development makes
> the slow structures more attractive. Every year there's an increase in
> the CPU and memory- access speed of the latest computers. Moore's Law
> (postulated by Gordon Moore in 1965) specifies that CPU performance
> will double every 18 months. This adds up to an astonishing difference
> in performance between the earliest computers and those available
> today, and there's no reason to think this increase will slow down any
> time soon.
>
> Suppose a computer a few years ago handled an array of 100 objects in
> acceptable time. Now, computers are much faster, so an array with
> 10,000 objects might run at the same speed. Many writers provide
> estimates of the maximum size you can make a data structure before it
> becomes too slow. Don't trust these estimates (including those in this
> book). Today's estimate doesn't apply to tomorrow.
>
> Instead, start by considering the simple data structures. Unless it's
> obvious they'll be too slow, code a simple version of an array or
> linked list and see what happens. If it runs in acceptable time, look
> no further. Why slave away on a balanced tree when no one would ever
> notice if you used an array instead? Even if you must deal with
> thousands or tens of thousands of items, it's still worthwhile to see
> how well an array or linked list will handle them. Only when
> experimentation shows their perfor- mance to be too slow should you
> revert to more sophisticated data structures.

####### Advantages of Java References

> Java has an advantage over some languages in the speed with which
> objects can be manipulated, because, in most data structures, Java
> stores only references, not actual objects. Therefore, most algorithms
> will run faster than in languages where actual objects occupy space in
> a data structure. In analyzing the algorithms, it's not the case, as
> when objects themselves are stored, that the time to "move" an object
> depends on the size of the object. Because only a reference is moved,
> it doesn't matter how large the object is.
>
> Of course, in other languages, such as C++, pointers to objects can be
> stored instead of the objects themselves; this has the same effect as
> using references, but the syntax is more complicated.

##### Libraries

> Libraries of data structures are available commercially in all major
> programming languages. Languages themselves may have some structures
> built in. Java, for example, includes Vector, Stack, and Hashtable
> classes. C++ includes the Standard Template Library (STL), which
> contains classes for many data structures and algo- rithms.
>
> 720 **CHAPTER 15** When to Use What
>
> Using a commercial library may eliminate or at least reduce the
> programming neces- sary to create the data structures described in
> this book. When that's the case, using a complex structure such as a
> balanced tree, or a delicate algorithm such as quick- sort, becomes a
> more attractive possibility. However, you must ensure that the class
> can be adapted to your particular situation.

##### Arrays

> In many situations the array is the first kind of structure you should
> consider when storing and manipulating data. Arrays are useful when

-   The amount of data is reasonably small.

-   The amount of data is predictable in advance.

> If you have plenty of memory, you can relax the second condition; just
> make the array big enough to handle any foreseeable influx of data.
>
> If insertion speed is important, use an unordered array. If search
> speed is important, use an ordered array with a binary search.
> Deletion is always slow in arrays because an average of half the items
> must be moved to fill in the newly vacated cell.
>
> Traversal is fast in an ordered array but not supported in an
> unordered array.
>
> Vectors, such as the Vector class supplied with Java, are arrays that
> expand them- selves when they become too full. Vectors may work when
> the amount of data isn't known in advance. However, there may
> periodically be a significant pause while they enlarge themselves by
> copying the old data into the new space.

##### Linked Lists

> Consider a linked list whenever the amount of data to be stored cannot
> be predicted in advance or when data will frequently be inserted and
> deleted. The linked list obtains whatever storage it needs as new
> items are added, so it can expand to fill all of available memory; and
> there is no need to fill "holes" during deletion, as there is in
> arrays.
>
> Insertion is fast in an unordered list. Searching and deletion are
> slow (although dele- tion is faster than in an array), so, like
> arrays, linked lists are best used when the amount of data is
> comparatively small.
>
> A linked list is somewhat more complicated to program than an array,
> but is simple compared with a tree or hash table.

##### Binary Search Trees

> A binary tree is the first structure to consider when arrays and
> linked lists prove too slow. A tree provides fast O(logN) insertion,
> searching, and deletion. Traversal is
>
> General-Purpose Data Structures 721
>
> O(N), which is the maximum for any data structure (by definition, you
> must visit every item). You can also find the minimum and maximum
> quickly and traverse a range of items.
>
> An unbalanced binary tree is much easier to program than a balanced
> tree, but unfortunately ordered data can reduce its performance to
> O(N) time, no better than a linked list. However, if you're sure the
> data will arrive in random order, there's no point using a balanced
> tree.

##### Balanced Trees

> Of the various kinds of balanced trees, we discussed red-black trees
> and 2-3-4 trees. They are both balanced trees, and thus guarantee
> O(logN) performance whether the input data is ordered or not. However,
> these balanced trees are challenging to program, with the red-black
> tree being the more difficult. They also impose addi- tional memory
> overhead, which may or may not be significant.
>
> The problem of complex programming may be reduced if a commercial
> class can be used for a tree. In some cases a hash table may be a
> better choice than a balanced tree. Hash-table performance doesn't
> degrade when the data is ordered.
>
> There are other kinds of balanced trees, including AVL trees, splay
> trees, 2-3 trees, and so on, but they are not as commonly used as the
> red-black tree.

##### Hash Tables

> Hash tables are the fastest data storage structure. This makes them a
> necessity for situations in which a computer program, rather than a
> human, is interacting with the data. Hash tables are typically used in
> spelling checkers and as symbol tables in computer language compilers,
> where a program must check thousands of words or symbols in a fraction
> of a second.
>
> Hash tables may also be useful when a person, as opposed to a
> computer, initiates data-access operations. As noted earlier, hash
> tables are not sensitive to the order in which data is inserted, and
> so can take the place of a balanced tree. Programming is much simpler
> than for balanced trees.
>
> Hash tables require additional memory, especially for open addressing.
> Also, the amount of data to be stored must be known fairly accurately
> in advance, because an array is used as the underlying structure.
>
> A hash table with separate chaining is the most robust implementation,
> unless the amount of data is known accurately in advance, in which
> case open addressing offers simpler programming because no linked list
> class is required.
>
> Hash tables don't support any kind of ordered traversal, or access to
> the minimum or maximum items. If these capabilities are important, the
> binary search tree is a better choice.
>
> 722 **CHAPTER 15** When to Use What

##### Comparing the General-Purpose Storage Structures

> Table 15.1 summarizes the speeds of the various general-purpose data
> storage structures using Big O notation.
>
> ***TABLE 15.1*** General-Purpose Data Storage Structures

+-------------+-------------+-------------+-------------+-------------+
| **Data      | > **Search* | > **Inserti | > **Deletio | > **Travers |
| Structure** | *           | on**        | n**         | al**        |
+=============+=============+=============+=============+=============+
| Array       | > O(N)      | > O(1)      | > O(N)      | > ---       |
+-------------+-------------+-------------+-------------+-------------+
| Ordered     | > O(logN)   | > O(N)      | > O(N)      | > O(N)      |
| array       |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+
| Linked list | > O(N)      | > O(1)      | > O(N)      | > ---       |
+-------------+-------------+-------------+-------------+-------------+
| Ordered     | > O(N)      | > O(N)      | > O(N)      | > O(N)      |
| linked list |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+
| Binary tree | > O(logN)   | > O(logN)   | > O(logN)   | > O(N)      |
| (average)   |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+
| Binary tree | > O(N)      | > O(N)      | > O(N)      | > O(N)      |
| (worst      |             |             |             |             |
| case)       |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+
| Balanced    | > O(logN)   | > O(logN)   | > O(logN)   | > O(N)      |
| tree        |             |             |             |             |
| (average    |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+
| and worst   |             |             |             |             |
| case)       |             |             |             |             |
+-------------+-------------+-------------+-------------+-------------+
| Hash table  | > O(1)      | > O(1)      | > O(1)      | > ---       |
+-------------+-------------+-------------+-------------+-------------+

> Insertion in an unordered array is assumed to be at the end of the
> array. The ordered array uses a binary search, which is fast, but
> insertion and deletion require moving half the items on the average,
> which is slow. Traversal implies visiting the data items in order of
> ascending or descending keys; the --- means this operation is not
> supported.

#### Special-Purpose Data Structures

> The special-purpose data structures discussed in this book are the
> stack, the queue, and the priority queue. These structures, rather
> than supporting a database of user- accessible data, are usually used
> by a computer program to aid in carrying out some algorithm. We've
> seen examples of this throughout this book, such as in Chapters 13,
> "Graphs," and 14, "Weighted Graphs," where stacks, queues, and
> priority queues are all used in graph algorithms.
>
> Stacks, queues, and priority queues are Abstract Data Types (ADTs)
> that are imple- mented by a more fundamental structure such as an
> array, linked list, or (in the case of the priority queue) a heap.
> These ADTs present a simple interface to the user, typi- cally
> allowing only insertion and the ability to access or delete only one
> data item. These items are

-   For stacks: the last item inserted

-   For queues: the first item inserted

-   For priority queues: the item with the highest priority

> Special-Purpose Data Structures 723
>
> These ADTs can be seen as conceptual aids. Their functionality could
> be obtained using the underlying structure (such as an array)
> directly, but the reduced interface they offer simplifies many
> problems.
>
> These ADTs can't be conveniently searched for an item by key value or
> traversed.

##### Stack

> A stack is used when you want access only to the last data item
> inserted; it's a Last- In-First-Out (LIFO) structure.
>
> A stack is often implemented as an array or a linked list. The array
> implementation is efficient because the most recently inserted item is
> placed at the end of the array, where it's also easy to delete. Stack
> overflow can occur, but is not likely if the array is reasonably
> sized, because stacks seldom contain huge amounts of data.
>
> If the stack will contain a lot of data and the amount can't be
> predicted accurately in advance (as when recursion is implemented as a
> stack), a linked list is a better choice than an array. A linked list
> is efficient because items can be inserted and deleted quickly from
> the head of the list. Stack overflow can't occur (unless the entire
> memory is full). A linked list is slightly slower than an array
> because memory alloca- tion is necessary to create a new link for
> insertion, and deallocation of the link is necessary at some point
> following removal of an item from the list.

##### Queue

> A queue is used when you want access only to the first data item
> inserted; it's a First-In-First-Out (FIFO) structure.
>
> Like stacks, queues can be implemented as arrays or linked lists. Both
> are efficient. The array requires additional programming to handle the
> situation in which the queue wraps around at the end of the array. A
> linked list must be double-ended, to allow insertions at one end and
> deletions at the other.
>
> As with stacks, the choice between an array implementation and a
> linked list imple- mentation is determined by how well the amount of
> data can be predicted. Use the array if you know about how much data
> there will be; otherwise, use a linked list.

##### Priority Queue

> A priority queue is used when the only access desired is to the data
> item with the highest priority. This is the item with the largest (or
> sometimes the smallest) key.
>
> Priority queues can be implemented as an ordered array or as a heap.
> Insertion into an ordered array is slow, but deletion is fast. With
> the heap implementation, both insertion and deletion take O(logN)
> time.
>
> 724 **CHAPTER 15** When to Use What
>
> Use an array or a double-ended linked list if insertion speed is not a
> problem. The array works when the amount of data to be stored can be
> predicted in advance; the linked list when the amount of data is
> unknown. If speed is important, a heap is a better choice.

##### Comparison of Special-Purpose Structures

> Table 15.2 shows the Big O times for stacks, queues, and priority
> queues. These structures don't support searching or traversal.
>
> ***TABLE 15.2*** Special-Purpose Data Storage Structures

+-----------------+-----------------+-----------------+-----------------+
| **Data          | > **Insertion** | > **Deletion**  | > **Comment**   |
| Structure**     |                 |                 |                 |
+=================+=================+=================+=================+
| Stack (array or | > O(1)          | > O(1)          | > Deletes most  |
|                 |                 |                 | > recently      |
+-----------------+-----------------+-----------------+-----------------+
| linked list)    |                 |                 | > inserted item |
+-----------------+-----------------+-----------------+-----------------+
| Queue (array or | > O(1)          | > O(1)          | > Deletes least |
|                 |                 |                 | > recently      |
+-----------------+-----------------+-----------------+-----------------+
| linked list)    |                 |                 | > inserted item |
+-----------------+-----------------+-----------------+-----------------+
| Priority queue  | > O(N)          | > O(1)          | > Deletes       |
|                 |                 |                 | > highest-prior |
|                 |                 |                 | ity             |
+-----------------+-----------------+-----------------+-----------------+
| (ordered array) |                 |                 | > item          |
+-----------------+-----------------+-----------------+-----------------+
| Priority queue  | > O(logN)       | > O(logN)       | > Deletes       |
|                 |                 |                 | > highest-prior |
|                 |                 |                 | ity             |
+-----------------+-----------------+-----------------+-----------------+
| (heap)          |                 |                 | > item          |
+-----------------+-----------------+-----------------+-----------------+

#### Sorting

> As with the choice of data structures, it's worthwhile initially to
> try a slow but simple sort, such as the insertion sort. It may be that
> the fast processing speeds available in modern computers will allow
> sorting of your data in reasonable time. (As a wild guess, the slow
> sort might be appropriate for fewer than 1,000 items.)
>
> Insertion sort is also good for almost-sorted files, operating in
> about O(N) time if not too many items are out of place. This is
> typically the case where a few new items are added to an
> already-sorted file.
>
> If the insertion sort proves too slow, then the Shellsort is the next
> candidate. It's fairly easy to implement, and not very temperamental.
> Sedgewick estimates it to be useful up to 5,000 items.
>
> Only when the Shellsort proves too slow should you use one of the more
> complex but faster sorts: mergesort, heapsort, or quicksort. Mergesort
> requires extra memory, heapsort requires a heap data structure, and
> both are somewhat slower than quick- sort, so quicksort is the usual
> choice when the fastest sorting time is necessary.
>
> However, quicksort is suspect if there's a danger that the data may
> not be random, in which case it may deteriorate to O(N2) performance.
> For potentially non-random

External Storage 725

> data, heapsort is better. Quicksort is also prone to subtle errors if
> it is not imple- mented correctly. Small mistakes in coding can make
> it work poorly for certain arrangements of data, a situation that may
> be hard to diagnose.
>
> Table 15.3 summarizes the running time for various sorting algorithms.
> The column labeled Comparison attempts to estimate the minor speed
> differences between algo- rithms with the same average Big O times.
> (There's no entry for Shellsort because there are no other algorithms
> with the same Big O performance.)
>
> ***TABLE 15.3*** Comparison of Sorting Algorithms

+-----------+---------------+--------------+------------------+--------------------+
| **Sort**  | > **Average** | > **Worst**  | > **Comparison** | > **Extra Memory** |
+===========+===============+==============+==================+====================+
| Bubble    | > O(N2)       | > O(N2)      | > Poor           | > No               |
+-----------+---------------+--------------+------------------+--------------------+
| Selection | > O(N2)       | > O(N2)      | > Fair           | > No               |
+-----------+---------------+--------------+------------------+--------------------+
| Insertion | > O(N2)       | > O(N2)      | > Good           | > No               |
+-----------+---------------+--------------+------------------+--------------------+
| Shellsort | > O(N3/2)     | > O(N3/2)    | > ---            | > No               |
+-----------+---------------+--------------+------------------+--------------------+
| Quicksort | > O(N\*logN)  | > O(N2)      | > Good           | > No               |
+-----------+---------------+--------------+------------------+--------------------+
| Mergesort | > O(N\*logN)  | > O(N\*logN) | > Fair           | > Yes              |
+-----------+---------------+--------------+------------------+--------------------+
| Heapsort  | > O(N\*logN)  | > O(N\*logN) | > Fair           | > No               |
+-----------+---------------+--------------+------------------+--------------------+

#### Graphs

> Graphs are unique in the pantheon of data storage structures. They
> don't store general-purpose data, and they don't act as programmer's
> tools for use in other algo- rithms. Instead, they directly model
> real-world situations. The structure of the graph reflects the
> structure of the problem.
>
> When you need a graph, nothing else will do, so there's no decision to
> be made about when to use one. The primary choice is how to represent
> the graph: using an adjacency matrix or adjacency lists. Your choice
> depends on whether the graph is full, when the adjacency matrix is
> preferred, or sparse, when the adjacency list should be used.
>
> The depth-first search and breadth-first search run in O(V2) time,
> where V is the number of vertices, for adjacency matrix
> representation. They run in O(V+E) time, where E is the number of
> edges, for adjacency list representation. Minimum span- ning trees and
> shortest paths run in O(V2) time using an adjacency matrix and
> O((E+V)logV) time using adjacency lists. You'll need to estimate V and
> E for your graph and do the arithmetic to see which representation is
> appropriate.

#### External Storage

> In the previous discussion we assumed that data was stored in main
> memory. However, amounts of data too large to store in memory must be
> stored in external
>
> 726 **CHAPTER 15** When to Use What
>
> storage, which generally means disk files. We discussed external
> storage in the second parts of Chapter 10, "2-3-4 Trees and External
> Storage," and Chapter 11, "Hash Tables."
>
> We assumed that data is stored in a disk file in fixed-size units
> called blocks, each of which holds a number of records. (A record in a
> disk file holds the same sort of data as an object in main memory.)
> Like an object, a record has a key value used to access it.
>
> We also assumed that reading and writing operations always involve a
> single block, and these read and write operations are far more
> time-consuming than any process- ing of data in main memory. Thus, for
> fast operation the number of disk accesses must be minimized.

##### Sequential Storage

> The simplest approach is to store records randomly and read them
> sequentially when searching for one with a particular key. New records
> can simply be inserted at the end of the file. Deleted records can be
> marked as deleted, or records can be shifted down (as in an array) to
> fill in the gap.
>
> On the average, searching and deletion will involve reading half the
> blocks, so sequential storage is not very fast, operating in O(N)
> time. Still, it might be satisfac- tory for a small number of records.

##### Indexed Files

> Speed is increased dramatically when indexed files are used. In this
> scheme an index of keys and corresponding block numbers is kept in
> main memory. To access a record with a specified key, the index is
> consulted. It supplies the block number for the key, and only one
> block needs to be read, taking O(1) time.
>
> Several indices with different kinds of keys can be used (one for last
> names, one for Social Security numbers, and so on). This scheme works
> well until the index becomes too large to fit in memory.
>
> Typically, the index files are themselves stored on disk and read into
> memory as needed.
>
> The disadvantage of indexed files is that at some point the index must
> be created. This probably involves reading through the file
> sequentially, so creating the index is slow. Also, the index will need
> to be updated when items are added to the file.

##### B-trees

> B-trees are multiway trees, commonly used in external storage, in
> which nodes corre- spond to blocks on the disk. As in other trees, the
> algorithms find their way down the tree, reading one block at each
> level. B-trees provide searching, insertion, and

External Storage 727

> deletion of records in O(logN) time. This is quite fast and works even
> for very large files. However, the programming is not trivial.

##### Hashing

> If it's acceptable to use about twice as much external storage as a
> file would normally take, then external hashing might be a good
> choice. It has the same access time as indexed files, O(1), but can
> handle larger files.
>
> Figure 15.2 shows, rather impressionistically, these choices for
> external storage structures.
>
> ***FIGURE 15.2*** Relationship of external storage choices.

##### Virtual Memory

> Sometimes you can let your operating system's virtual memory
> capabilities (if it has them) solve disk access problems with little
> programming effort on your part.
>
> If you read a file that's too big to fit in main memory, the virtual
> memory system will read in that part of the file that fits and store
> the rest on the disk. As you access different parts of the file, they
> will be read from the disk automatically and placed in memory.
>
> 728 **CHAPTER 15** When to Use What
>
> You can apply internal algorithms to the entire file just as if it was
> all in memory at the same time, and let the operating system worry
> about reading the appropriate part of the file if it isn't in memory
> already.
>
> Of course, operation will be much slower than when the entire file is
> in memory, but this would also be true if you dealt with the file
> block by block using one of the external-storage algorithms. It may be
> worth simply ignoring the fact that a file doesn't fit in memory and
> seeing how well your algorithms work with the help of virtual memory.
> Especially for files that aren't much larger than the available
> memory, this may be an easy solution.

#### Onward

> We've come to the end of our survey of data structures and algorithms.
> The subject is large and complex, so no one book can make you an
> expert, but we hope this book has made it easy for you to learn about
> the fundamentals. Appendix B, "Further Reading," contains suggestions
> for further study.
